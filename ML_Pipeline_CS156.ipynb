{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8600b1f",
   "metadata": {},
   "source": [
    "# Music Genre Classification from Spotify Streaming History\n",
    "## CS156 Machine Learning Pipeline Project\n",
    "\n",
    "This notebook implements a machine learning pipeline to classify music genres using personal Spotify streaming history data. The project demonstrates the complete machine learning lifecycle from data collection to model evaluation.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Data Collection and Description](#1.-Data-Collection-and-Description)\n",
    "2. [Data Loading and Conversion](#2.-Data-Loading-and-Conversion)\n",
    "3. [Data Preprocessing and EDA](#3.-Data-Preprocessing-and-EDA)\n",
    "4. [Analysis Planning](#4.-Analysis-Planning)\n",
    "5. [Model Selection](#5.-Model-Selection)\n",
    "6. [Model Training](#6.-Model-Training)\n",
    "7. [Model Evaluation](#7.-Model-Evaluation)\n",
    "8. [Results Visualization](#8.-Results-Visualization)\n",
    "9. [Executive Summary](#9.-Executive-Summary)\n",
    "10. [References](#10.-References)\n",
    "\n",
    "Let's begin by setting up our environment and importing required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abd932a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import librosa\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd0e260",
   "metadata": {},
   "source": [
    "# 1. Data Collection and Description\n",
    "\n",
    "## Dataset Overview\n",
    "\n",
    "The dataset used in this project consists of personal Spotify streaming history data. This data includes:\n",
    "\n",
    "1. Streaming history JSON files from Spotify\n",
    "2. Audio preview files for feature extraction\n",
    "3. Audio features and characteristics\n",
    "\n",
    "### Data Sources\n",
    "\n",
    "1. **Spotify Account Data**: \n",
    "   - Personal streaming history downloaded from Spotify\n",
    "   - Contains track names, artists, and listening timestamps\n",
    "   - Format: JSON files (`StreamingHistory*.json`)\n",
    "\n",
    "2. **Audio Previews**:\n",
    "   - 30-second preview clips of tracks\n",
    "   - Used for extracting audio features\n",
    "   - Format: MP3 files\n",
    "\n",
    "### Sampling Methodology\n",
    "\n",
    "The data represents my personal listening history from 2023 to 2025, providing a comprehensive view of my music preferences. This temporal range ensures:\n",
    "\n",
    "1. Recent listening patterns are captured\n",
    "2. Sufficient data for meaningful analysis\n",
    "3. Diverse genre representation\n",
    "\n",
    "Let's start by examining our raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27738731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load streaming history from JSON files\n",
    "def load_streaming_history(file_path):\n",
    "    \"\"\"Load and parse Spotify streaming history JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        return pd.DataFrame(data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Load the streaming history data\n",
    "file_path = 'Streaming_History_Audio_2023-2025_0.json'\n",
    "streaming_df = load_streaming_history(file_path)\n",
    "\n",
    "if streaming_df is not None:\n",
    "    print(\"Dataset Overview:\")\n",
    "    print(f\"Number of entries: {len(streaming_df)}\")\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    display(streaming_df.head())\n",
    "    print(\"\\nColumns:\", streaming_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fb730d",
   "metadata": {},
   "source": [
    "# 2. Data Loading and Conversion\n",
    "\n",
    "In this section, we'll convert our raw data into a format suitable for machine learning. This involves:\n",
    "\n",
    "1. Parsing JSON data into structured DataFrame\n",
    "2. Converting timestamps and durations\n",
    "3. Creating a proper data structure for analysis\n",
    "\n",
    "## Data Loading Pipeline\n",
    "\n",
    "We'll create a `DataLoader` class to handle the data loading and initial processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c998fc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    \"\"\"Class for loading and initial processing of Spotify streaming data.\"\"\"\n",
    "    \n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.data = None\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"Load JSON data into pandas DataFrame.\"\"\"\n",
    "        try:\n",
    "            with open(self.file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            self.data = pd.DataFrame(data)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def parse_timestamps(self):\n",
    "        \"\"\"Convert timestamp strings to datetime objects.\"\"\"\n",
    "        if self.data is not None:\n",
    "            self.data['endTime'] = pd.to_datetime(self.data['endTime'])\n",
    "            self.data['date'] = self.data['endTime'].dt.date\n",
    "            self.data['hour'] = self.data['endTime'].dt.hour\n",
    "            self.data['day_of_week'] = self.data['endTime'].dt.day_name()\n",
    "    \n",
    "    def convert_durations(self):\n",
    "        \"\"\"Convert milliseconds played to minutes and seconds.\"\"\"\n",
    "        if self.data is not None:\n",
    "            self.data['secondsPlayed'] = self.data['msPlayed'] / 1000\n",
    "            self.data['minutesPlayed'] = self.data['secondsPlayed'] / 60\n",
    "    \n",
    "    def process_data(self):\n",
    "        \"\"\"Run all processing steps.\"\"\"\n",
    "        if self.load_data():\n",
    "            self.parse_timestamps()\n",
    "            self.convert_durations()\n",
    "            return self.data\n",
    "        return None\n",
    "\n",
    "# Create data loader instance and process data\n",
    "loader = DataLoader('Streaming_History_Audio_2023-2025_0.json')\n",
    "processed_df = loader.process_data()\n",
    "\n",
    "if processed_df is not None:\n",
    "    print(\"Processed Data Overview:\")\n",
    "    print(f\"Number of entries: {len(processed_df)}\")\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    display(processed_df.head())\n",
    "    print(\"\\nColumns:\", processed_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471a89b7",
   "metadata": {},
   "source": [
    "# 3. Data Preprocessing and EDA\n",
    "\n",
    "In this section, we'll clean our data and perform exploratory data analysis. This includes:\n",
    "\n",
    "1. Data cleaning\n",
    "2. Feature engineering\n",
    "3. Basic statistics and visualizations\n",
    "\n",
    "## Data Cleaning\n",
    "\n",
    "Let's start by cleaning our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793ec6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCleaner:\n",
    "    \"\"\"Class for cleaning and preprocessing streaming data.\"\"\"\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.df = df.copy()\n",
    "    \n",
    "    def remove_short_plays(self, min_seconds=30):\n",
    "        \"\"\"Remove tracks played for less than min_seconds.\"\"\"\n",
    "        initial_count = len(self.df)\n",
    "        self.df = self.df[self.df['secondsPlayed'] >= min_seconds]\n",
    "        removed = initial_count - len(self.df)\n",
    "        print(f\"Removed {removed} short plays (<{min_seconds}s)\")\n",
    "    \n",
    "    def clean_names(self):\n",
    "        \"\"\"Clean artist and track names.\"\"\"\n",
    "        self.df['artistName'] = self.df['artistName'].str.strip()\n",
    "        self.df['trackName'] = self.df['trackName'].str.strip()\n",
    "    \n",
    "    def remove_duplicates(self):\n",
    "        \"\"\"Remove duplicate entries based on timestamp and track info.\"\"\"\n",
    "        initial_count = len(self.df)\n",
    "        self.df = self.df.drop_duplicates(\n",
    "            subset=['endTime', 'artistName', 'trackName'],\n",
    "            keep='first'\n",
    "        )\n",
    "        removed = initial_count - len(self.df)\n",
    "        print(f\"Removed {removed} duplicate entries\")\n",
    "    \n",
    "    def clean_data(self):\n",
    "        \"\"\"Run all cleaning steps.\"\"\"\n",
    "        self.remove_short_plays()\n",
    "        self.clean_names()\n",
    "        self.remove_duplicates()\n",
    "        return self.df\n",
    "\n",
    "# Clean the data\n",
    "cleaner = DataCleaner(processed_df)\n",
    "cleaned_df = cleaner.clean_data()\n",
    "\n",
    "print(\"\\nCleaned Data Overview:\")\n",
    "print(f\"Final number of entries: {len(cleaned_df)}\")\n",
    "display(cleaned_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7242f1ac",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "Let's analyze our cleaned dataset to understand patterns and distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87105bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_listening_patterns(df):\n",
    "    \"\"\"Create visualizations of listening patterns.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Listening time by hour\n",
    "    hourly_counts = df.groupby('hour').size()\n",
    "    sns.barplot(x=hourly_counts.index, y=hourly_counts.values, ax=axes[0,0])\n",
    "    axes[0,0].set_title('Listening Activity by Hour')\n",
    "    axes[0,0].set_xlabel('Hour of Day')\n",
    "    axes[0,0].set_ylabel('Number of Tracks')\n",
    "    \n",
    "    # 2. Listening time by day of week\n",
    "    day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    daily_counts = df.groupby('day_of_week').size()\n",
    "    daily_counts = daily_counts.reindex(day_order)\n",
    "    sns.barplot(x=daily_counts.index, y=daily_counts.values, ax=axes[0,1])\n",
    "    axes[0,1].set_title('Listening Activity by Day of Week')\n",
    "    axes[0,1].set_xlabel('Day of Week')\n",
    "    axes[0,1].set_ylabel('Number of Tracks')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # 3. Distribution of track durations\n",
    "    sns.histplot(data=df, x='minutesPlayed', bins=50, ax=axes[1,0])\n",
    "    axes[1,0].set_title('Distribution of Track Durations')\n",
    "    axes[1,0].set_xlabel('Minutes Played')\n",
    "    axes[1,0].set_ylabel('Count')\n",
    "    \n",
    "    # 4. Top artists\n",
    "    top_artists = df['artistName'].value_counts().head(10)\n",
    "    sns.barplot(x=top_artists.values, y=top_artists.index, ax=axes[1,1])\n",
    "    axes[1,1].set_title('Top 10 Most Played Artists')\n",
    "    axes[1,1].set_xlabel('Number of Tracks')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate visualizations\n",
    "plot_listening_patterns(cleaned_df)\n",
    "\n",
    "# Print basic statistics\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(f\"Total unique tracks: {cleaned_df['trackName'].nunique()}\")\n",
    "print(f\"Total unique artists: {cleaned_df['artistName'].nunique()}\")\n",
    "print(f\"Average listening time per track: {cleaned_df['minutesPlayed'].mean():.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ed07b3",
   "metadata": {},
   "source": [
    "# 4. Analysis Planning\n",
    "\n",
    "Now that we have cleaned and analyzed our data, let's plan our genre classification approach:\n",
    "\n",
    "## Classification Task Definition\n",
    "\n",
    "We will build a music genre classifier using the following:\n",
    "\n",
    "1. **Input Features**:\n",
    "   - Audio features extracted using librosa\n",
    "   - Temporal features (tempo, rhythm)\n",
    "   - Spectral features (MFCC, spectral centroid, etc.)\n",
    "\n",
    "2. **Target Variable**:\n",
    "   - Music genre (multi-class classification)\n",
    "\n",
    "Let's prepare our feature extraction pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac11e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioFeatureExtractor:\n",
    "    \"\"\"Extract audio features from MP3 files using librosa.\"\"\"\n",
    "    \n",
    "    def __init__(self, sample_rate=22050):\n",
    "        self.sample_rate = sample_rate\n",
    "    \n",
    "    def load_audio(self, audio_path, duration=30):\n",
    "        \"\"\"Load audio file with specified duration.\"\"\"\n",
    "        try:\n",
    "            y, sr = librosa.load(audio_path, sr=self.sample_rate, duration=duration)\n",
    "            return y, sr\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading audio: {str(e)}\")\n",
    "            return None, None\n",
    "    \n",
    "    def extract_features(self, y, sr):\n",
    "        \"\"\"Extract various audio features.\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        try:\n",
    "            # Rhythm features\n",
    "            tempo, _ = librosa.beat.beat_track(y=y, sr=sr)\n",
    "            features['tempo'] = tempo\n",
    "            \n",
    "            # Spectral features\n",
    "            features['spectral_centroid'] = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr))\n",
    "            features['spectral_bandwidth'] = np.mean(librosa.feature.spectral_bandwidth(y=y, sr=sr))\n",
    "            features['spectral_rolloff'] = np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr))\n",
    "            \n",
    "            # MFCC features\n",
    "            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "            for i, mfcc in enumerate(mfccs):\n",
    "                features[f'mfcc_{i}'] = np.mean(mfcc)\n",
    "            \n",
    "            return features\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting features: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def process_audio_file(self, audio_path):\n",
    "        \"\"\"Process a single audio file and extract features.\"\"\"\n",
    "        y, sr = self.load_audio(audio_path)\n",
    "        if y is not None:\n",
    "            return self.extract_features(y, sr)\n",
    "        return None\n",
    "\n",
    "# Example usage (commented out as we don't have audio files yet)\n",
    "'''\n",
    "extractor = AudioFeatureExtractor()\n",
    "features = extractor.process_audio_file('path_to_audio.mp3')\n",
    "if features:\n",
    "    print(\"Extracted features:\", features)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5c0bf6",
   "metadata": {},
   "source": [
    "# 5. Model Selection\n",
    "\n",
    "For our genre classification task, we'll compare several models:\n",
    "\n",
    "1. **Random Forest Classifier**\n",
    "   - Ensemble method combining multiple decision trees\n",
    "   - Good for handling non-linear relationships\n",
    "   - Built-in feature importance\n",
    "\n",
    "2. **Support Vector Machine (SVM)**\n",
    "   - Effective for high-dimensional data\n",
    "   - Strong theoretical guarantees\n",
    "   - Kernel tricks for non-linear classification\n",
    "\n",
    "3. **Gradient Boosting Classifier**\n",
    "   - Sequential ensemble method\n",
    "   - Often achieves state-of-the-art results\n",
    "   - Good handling of imbalanced data\n",
    "\n",
    "Let's set up our model pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd891828",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "class GenreClassifier:\n",
    "    \"\"\"Music genre classification model pipeline.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_type='rf'):\n",
    "        self.model_type = model_type\n",
    "        self.pipeline = None\n",
    "        self.setup_pipeline()\n",
    "    \n",
    "    def setup_pipeline(self):\n",
    "        \"\"\"Create the model pipeline with preprocessing and classifier.\"\"\"\n",
    "        if self.model_type == 'rf':\n",
    "            clf = RandomForestClassifier(random_state=42)\n",
    "            param_grid = {\n",
    "                'classifier__n_estimators': [100, 200, 300],\n",
    "                'classifier__max_depth': [10, 20, 30, None]\n",
    "            }\n",
    "        elif self.model_type == 'svm':\n",
    "            clf = SVC(random_state=42)\n",
    "            param_grid = {\n",
    "                'classifier__C': [0.1, 1, 10],\n",
    "                'classifier__kernel': ['rbf', 'linear']\n",
    "            }\n",
    "        else:  # gradient boosting\n",
    "            clf = GradientBoostingClassifier(random_state=42)\n",
    "            param_grid = {\n",
    "                'classifier__n_estimators': [100, 200],\n",
    "                'classifier__learning_rate': [0.01, 0.1]\n",
    "            }\n",
    "        \n",
    "        self.pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('classifier', clf)\n",
    "        ])\n",
    "        \n",
    "        self.param_grid = param_grid\n",
    "    \n",
    "    def train(self, X_train, y_train, cv=5):\n",
    "        \"\"\"Train the model using cross-validation.\"\"\"\n",
    "        grid_search = GridSearchCV(\n",
    "            self.pipeline,\n",
    "            self.param_grid,\n",
    "            cv=cv,\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        self.pipeline = grid_search.best_estimator_\n",
    "        return grid_search.best_score_, grid_search.best_params_\n",
    "\n",
    "# Example usage (commented out as we don't have features yet)\n",
    "'''\n",
    "# Prepare data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features_matrix,\n",
    "    genre_labels,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train model\n",
    "classifier = GenreClassifier(model_type='rf')\n",
    "best_score, best_params = classifier.train(X_train, y_train)\n",
    "print(f\"Best CV score: {best_score}\")\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e85d41e",
   "metadata": {},
   "source": [
    "# 6. Model Training\n",
    "\n",
    "Once we have our features extracted and models set up, we'll train our models with cross-validation and hyperparameter tuning. The actual training will be performed when we have our feature matrix ready.\n",
    "\n",
    "For now, let's prepare the evaluation metrics we'll use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb36c749",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"Evaluate model performance with various metrics.\"\"\"\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage (commented out as we don't have trained model yet)\n",
    "'''\n",
    "evaluate_model(classifier.pipeline, X_test, y_test)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f5a3a5",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "\n",
    "To complete our machine learning pipeline, we need to:\n",
    "\n",
    "1. Extract audio features from our dataset\n",
    "2. Create the feature matrix and labels\n",
    "3. Train and evaluate our models\n",
    "4. Visualize results\n",
    "5. Write up conclusions and recommendations\n",
    "\n",
    "The next step is to implement the audio feature extraction pipeline. We'll do this by:\n",
    "\n",
    "1. Setting up the audio processing environment\n",
    "2. Downloading preview URLs for our tracks\n",
    "3. Extracting features from the audio files\n",
    "4. Creating our feature matrix\n",
    "\n",
    "Would you like to proceed with implementing these next steps?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
